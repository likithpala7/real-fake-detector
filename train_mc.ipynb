{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/likithpala7/real-fake-detector/blob/main/train_mc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZugPr9GJAST",
        "outputId": "a2477b60-8895-47fd-c047-d084152e7fe3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ftfy\n",
            "  Downloading ftfy-6.2.0-py3-none-any.whl (54 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/54.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.4/54.4 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.4)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy) (0.2.13)\n",
            "Installing collected packages: ftfy\n",
            "Successfully installed ftfy-6.2.0\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-ngw9xw42\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-ngw9xw42\n",
            "  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (6.2.0)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.66.4)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.3.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.18.0+cu121)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.13)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch->clip==1.0)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch->clip==1.0)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch->clip==1.0)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch->clip==1.0)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch->clip==1.0)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch->clip==1.0)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch->clip==1.0)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch->clip==1.0)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch->clip==1.0)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch->clip==1.0)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch->clip==1.0)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch->clip==1.0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->clip==1.0) (1.3.0)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369499 sha256=93ca6d8ce81b19a9ce799dd06c539f05802be90e3513f8958c51089ae3ea4a42\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-ns4nhbet/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n",
            "Successfully built clip\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, clip\n",
            "Successfully installed clip-1.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105\n",
            "Collecting peft\n",
            "  Downloading peft-0.11.1-py3-none-any.whl (251 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m251.6/251.6 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (24.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.3.0+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.41.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.66.4)\n",
            "Collecting accelerate>=0.21.0 (from peft)\n",
            "  Downloading accelerate-0.30.1-py3-none-any.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.6/302.6 kB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.23.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (3.14.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.3.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.13.0->peft) (12.5.40)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2023.12.25)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.19.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
            "Installing collected packages: accelerate, peft\n",
            "Successfully installed accelerate-0.30.1 peft-0.11.1\n"
          ]
        }
      ],
      "source": [
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "!pip install peft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fp-GoVH8JRrY",
        "outputId": "8d217648-ce4c-43f6-9a6d-e0296bbde1cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Hs2ZMb4fJZA4"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "import json\n",
        "import torch\n",
        "import clip\n",
        "import peft\n",
        "from PIL import Image\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import json\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from torch.utils.data import DataLoader\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from torchvision.ops import sigmoid_focal_loss\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple\n",
        "import pandas as pd\n",
        "from copy import deepcopy\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoProcessor, CLIPVisionModel, CLIPModel, CLIPVisionModelWithProjection\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "vWV2YZ0uK4DY"
      },
      "outputs": [],
      "source": [
        "!unzip -q drive/MyDrive/GenImage/imagenet_ai_holdout.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "GfgORd39K6fh"
      },
      "outputs": [],
      "source": [
        "!rm imagenet_ai_holdout/ADM/train/115_adm_156.PNG\n",
        "!rm imagenet_ai_holdout/BigGAN/train/116_biggan_00098.png\n",
        "!rm imagenet_ai_holdout/BigGAN/train/116_biggan_00107.png"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZiR_RfE-JbDI",
        "outputId": "32c3de5b-e49f-4a88-a068-4b69d35fc8ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'ADM': 0, 'BigGAN': 1, 'Glide': 2, 'SDV4': 3, 'SDV5': 4, 'VQDM': 5, 'nature': 6, 'wukong': 7}\n"
          ]
        }
      ],
      "source": [
        "label_dict = {model: i for i, model in\n",
        "               enumerate(sorted(os.listdir('imagenet_ai_holdout')))}\n",
        "print(label_dict)\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "\n",
        "    def get_dataset(self, d_type):\n",
        "\n",
        "        data = []\n",
        "\n",
        "        for dataset in os.listdir('imagenet_ai_holdout'):\n",
        "\n",
        "            imgs = os.listdir(os.path.join('imagenet_ai_holdout', dataset, d_type))\n",
        "\n",
        "            data.extend([os.path.join('imagenet_ai_holdout', dataset, d_type, img)\n",
        "                         for img in imgs])\n",
        "\n",
        "\n",
        "        return data\n",
        "\n",
        "\n",
        "    def __init__(self, config, d_type='train'):\n",
        "\n",
        "\n",
        "       self.data = self.get_dataset(d_type)\n",
        "       self.preprocess = AutoProcessor.from_pretrained('laion/CLIP-ViT-H-14-laion2B-s32B-b79K')\n",
        "\n",
        "       self.model = config.model\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        # Get the question and answer at the idx\n",
        "        img_path = self.data[idx]\n",
        "\n",
        "        model_type = img_path.split(os.path.sep)[1]\n",
        "        label = torch.tensor(label_dict[model_type]).to(device)\n",
        "\n",
        "        img = self.preprocess(images=Image.open(img_path),\n",
        "                              return_tensors='pt').pixel_values.squeeze(0).to(device)\n",
        "\n",
        "        return img, label\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DLAliDOUKs9N"
      },
      "outputs": [],
      "source": [
        "CLIP_HIDDEN_STATE = 512\n",
        "\n",
        "def print_trainable_parameters(model):\n",
        "\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "\n",
        "    print(\n",
        "        f\"Trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )\n",
        "\n",
        "class CLIPTeacher(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "\n",
        "        self.clip_model = CLIPVisionModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
        "\n",
        "        for param in self.clip_model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        hidden_state = self.clip_model.config.hidden_size\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_state, nhead=config.nhead, batch_first=True)\n",
        "        self.teacher_model = nn.TransformerEncoder(encoder_layer, num_layers=config.nlayers)\n",
        "        self.classifier = nn.Sequential(nn.Linear(hidden_state, hidden_state // 2),\n",
        "                                        nn.Dropout(0.2),\n",
        "                                        nn.ReLU(),\n",
        "                                        nn.Linear(hidden_state // 2, len(label_dict))\n",
        "                                        )\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Get hidden states from CLIP Model (N, S, H)\n",
        "        last_hidden_state = self.clip_model(x).last_hidden_state\n",
        "        out = self.teacher_model(last_hidden_state)\n",
        "\n",
        "        # Get the first token's hidden state\n",
        "        out = torch.mean(out, dim=1)\n",
        "\n",
        "        return self.classifier(out)\n",
        "\n",
        "    def get_features(self, x):\n",
        "        # Get hidden states from CLIP Model (N, S, H)\n",
        "        last_hidden_state = self.clip_model(x).last_hidden_state\n",
        "        out = self.teacher_model(last_hidden_state)\n",
        "\n",
        "        # Get the first token's hidden state\n",
        "        out = torch.mean(out, dim=1)\n",
        "\n",
        "        return self.classifier[:-1](out)\n",
        "\n",
        "\n",
        "class CLIPLarge(nn.Module):\n",
        "\n",
        "    def __init__(self, model_name):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.clip_model = CLIPVisionModelWithProjection.from_pretrained(model_name)\n",
        "        in_features = self.clip_model.config.projection_dim\n",
        "\n",
        "        lora_config = LoraConfig(\n",
        "            task_type=peft.TaskType.FEATURE_EXTRACTION,\n",
        "            # bias='lora_only',\n",
        "            # use_rslora=True,\n",
        "            target_modules=['q_proj', 'v_proj']\n",
        "        )\n",
        "\n",
        "        self.clip_model = get_peft_model(self.clip_model, lora_config)\n",
        "\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Linear(in_features, in_features // 2),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.fc2 = nn.Linear(in_features // 2, len(label_dict))\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        return self.fc2(self.fc1(self.clip_model(pixel_values=x).image_embeds.float().to(device)))\n",
        "\n",
        "    def get_features(self, x):\n",
        "\n",
        "        return self.fc1(self.clip_model(pixel_values=x).image_embeds.float().to(device))\n",
        "\n",
        "class CLIP_Model(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.clip_model, _ = clip.load('ViT-B/32', device=device)\n",
        "\n",
        "        # Freeze the CLIP model\n",
        "        for param in self.clip_model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.fc1 = nn.Sequential(\n",
        "            nn.Linear(CLIP_HIDDEN_STATE, CLIP_HIDDEN_STATE // 2),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "        self.fc2 = nn.Linear(CLIP_HIDDEN_STATE // 2, len(label_dict))\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.fc1(self.clip_model.encode_image(x).float()))\n",
        "\n",
        "    def get_features(self, x):\n",
        "        return self.fc1(self.clip_model.encode_image(x).float())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "kDaQyvA3K8F5"
      },
      "outputs": [],
      "source": [
        "Config = namedtuple('Instance', ['batch_size', 'learning_rate',\n",
        "                                 'weight_decay', 'num_workers',\n",
        "                                 'epochs', 'load_checkpoint',\n",
        "                                 'file_checkpoint', 'loss', 'model', 'nlayers',\n",
        "                                 'nhead', 'temperature', 'alpha'])\n",
        "\n",
        "config = Config(\n",
        "    batch_size = 16,\n",
        "    learning_rate = 1e-3,\n",
        "    weight_decay = 0.05,\n",
        "    num_workers = 0,\n",
        "    nlayers=2,\n",
        "    nhead=4,\n",
        "    epochs = 1,\n",
        "    loss = 'crossentropy',\n",
        "    load_checkpoint = False,\n",
        "    model = 'clip-large',\n",
        "    file_checkpoint = '',\n",
        "    temperature = 15,\n",
        "    alpha = 0.7\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzS1hkELK91S",
        "outputId": "42cdf017-a63a-4ca2-9991-e42705c4b04d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All model checkpoints and training stats will be saved in drive/MyDrive/GenImage/results/20240526-234137\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trainable params: 1839624 || all params: 633916424 || trainable%: 0.29019976929955676\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------------------- EPOCH 0 ---------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/16835 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.0562,  0.1635, -0.1006,  0.0313, -0.0242,  0.0019,  0.0678, -0.2437],\n",
            "        [ 0.0103,  0.0011, -0.0501,  0.0625,  0.3765,  0.1374, -0.0588,  0.0004],\n",
            "        [ 0.1399,  0.0782, -0.2779,  0.0552, -0.1852, -0.0246,  0.0737,  0.0410],\n",
            "        [-0.1244,  0.0162, -0.1230, -0.2458,  0.0628,  0.3535,  0.0146, -0.2200],\n",
            "        [ 0.0867,  0.0827, -0.0317, -0.0377, -0.0407,  0.0491,  0.0343,  0.2602],\n",
            "        [ 0.1890,  0.2289, -0.1104, -0.2284,  0.1069, -0.1153, -0.2524, -0.0986],\n",
            "        [ 0.1512, -0.0608,  0.0149,  0.1743,  0.0256,  0.1058, -0.1376, -0.2482],\n",
            "        [ 0.0026,  0.1878,  0.1849, -0.0472,  0.2179, -0.1278, -0.0229, -0.2066],\n",
            "        [ 0.0474, -0.0644, -0.1596,  0.1355, -0.0967,  0.0032,  0.1255, -0.0084],\n",
            "        [ 0.1813,  0.3699, -0.0209,  0.0117,  0.1431, -0.0824, -0.0089,  0.0265],\n",
            "        [ 0.1673,  0.0627,  0.0998, -0.1201, -0.1954,  0.3441,  0.0732, -0.1796],\n",
            "        [ 0.1728,  0.0697, -0.0678, -0.0490,  0.2044,  0.1700, -0.1279, -0.2943],\n",
            "        [ 0.1264,  0.2228, -0.0607,  0.1036,  0.3974,  0.0926, -0.1381, -0.0855],\n",
            "        [-0.0993,  0.0230, -0.3456,  0.0017, -0.1472,  0.0097, -0.2183, -0.3651],\n",
            "        [ 0.3017,  0.2414,  0.1418,  0.0251,  0.2100,  0.0087, -0.0565,  0.0200],\n",
            "        [ 0.1238, -0.2388,  0.1514,  0.1037, -0.1583,  0.1150, -0.2101, -0.1252]],\n",
            "       device='cuda:0', grad_fn=<AddmmBackward0>)\n",
            "tensor([3, 0, 2, 6, 2, 2, 7, 5, 3, 4, 4, 6, 5, 0, 1, 4], device='cuda:0')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 38/16835 [02:29<18:33:59,  3.98s/it]"
          ]
        }
      ],
      "source": [
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "\n",
        "    print(\n",
        "        f\"Trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )\n",
        "\n",
        "def save_model(model, model_name):\n",
        "    # Save the model into the designated folder\n",
        "    path = os.path.join('drive', 'MyDrive', 'GenImage', 'results', timestr, model_name + '.pth')\n",
        "    torch.save(model, path)\n",
        "\n",
        "def loss_fn_kd(outputs, labels, teacher_outputs):\n",
        "    \"\"\"\n",
        "    Compute the knowledge-distillation (KD) loss given outputs, labels.\n",
        "    \"Hyperparameters\": temperature and alpha\n",
        "\n",
        "    NOTE: the KL Divergence for PyTorch comparing the softmaxs of teacher\n",
        "    and student expects the input tensor to be log probabilities! See Issue #2\n",
        "    \"\"\"\n",
        "    alpha = config.alpha\n",
        "    T = config.temperature\n",
        "    KD_loss = torch.nn.KLDivLoss()(F.log_softmax(outputs/T, dim=1),\n",
        "                             F.softmax(teacher_outputs/T, dim=1)) * (alpha * T * T) + \\\n",
        "              F.cross_entropy(outputs, labels) * (1. - alpha)\n",
        "\n",
        "    return KD_loss\n",
        "\n",
        "def val_model(dloader, val_model):\n",
        "    val_model.eval()\n",
        "    val_loss = 0\n",
        "    val_accuracy = 0\n",
        "    predictions, label_list = [], []\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "      for idx, (inputs, labels) in tqdm(enumerate(dloader), total=len(dloader)):\n",
        "\n",
        "          logits = val_model(inputs)\n",
        "          loss = criterion(logits, labels)\n",
        "\n",
        "          val_loss += loss.item()\n",
        "\n",
        "          val_acc, preds = compute_accuracy(logits, labels)\n",
        "          val_accuracy += val_acc\n",
        "          predictions.extend(preds.tolist())\n",
        "          label_list.extend(labels.tolist())\n",
        "\n",
        "    plot_confusion_matrix(predictions, label_list)\n",
        "    return val_loss / len(val_dataloader), val_accuracy / len(val_dataloader)\n",
        "\n",
        "def save_stats(train_loss, val_loss, train_acc, val_acc, epochs,\n",
        "               lr, train_accs, val_accs):\n",
        "    stats_dict = {\n",
        "        'losses': losses,\n",
        "        'val losses': val_losses,\n",
        "        'training accuracies': train_accs,\n",
        "        'val accuracies': val_accs,\n",
        "        'min train loss': train_loss,\n",
        "        'min val loss': val_loss,\n",
        "        'max train acc': train_acc,\n",
        "        'max val acc': val_acc,\n",
        "        'epochs': epochs,\n",
        "        'learning rate': lr,\n",
        "        'loss': config.loss\n",
        "    }\n",
        "\n",
        "    fname = f'stats.json'\n",
        "\n",
        "    # Save stats into checkpoint\n",
        "    with open(os.path.join('drive', 'MyDrive', 'GenImage', 'results', timestr, fname), 'w') as f:\n",
        "        json.dump(stats_dict, f)\n",
        "\n",
        "def compute_accuracy(logits, labels):\n",
        "\n",
        "    preds = torch.argmax(F.softmax(logits, dim=-1), dim=-1)\n",
        "    correct = (preds == labels).sum().item()\n",
        "    total = labels.size(0)\n",
        "    return correct / total, preds\n",
        "\n",
        "def plot_confusion_matrix(predictions, labels):\n",
        "\n",
        "    # Define the class names and order\n",
        "    classes = sorted(os.listdir('imagenet_ai_holdout'))\n",
        "\n",
        "    # Create the confusion matrix using sklearn\n",
        "    cm = confusion_matrix(labels, predictions)\n",
        "\n",
        "    # Calculate row-wise sums to normalize the confusion matrix\n",
        "    row_sums = cm.sum(axis=1, keepdims=True)\n",
        "    normalized_cm = cm / row_sums.astype(float)\n",
        "\n",
        "    # Convert normalized confusion matrix to DataFrame with named rows and columns\n",
        "    normalized_cm_df = pd.DataFrame(normalized_cm, index=classes, columns=classes)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.set(font_scale=1.4)  # Adjust to fit labels properly\n",
        "\n",
        "    # Create a heatmap plot\n",
        "    sns.heatmap(normalized_cm_df, annot=True, fmt='.2f', cmap='Blues', cbar=False,\n",
        "                annot_kws={\"size\": 16}, linewidths=1, linecolor='black')\n",
        "\n",
        "    plt.title('Normalized Confusion Matrix')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.yticks(rotation=0)  # Ensure correct orientation of row labels\n",
        "\n",
        "    # Adjust layout to ensure all borders are visible\n",
        "    plt.tight_layout(pad=1.0)\n",
        "\n",
        "    img_num = len(os.listdir(os.path.join('drive', 'MyDrive', 'GenImage',\n",
        "                                          'results', timestr, 'Confusion Matrices')))\n",
        "    fname = f'cf_{img_num}.png'\n",
        "    plt.savefig(os.path.join('drive', 'MyDrive', 'GenImage', 'results', timestr, 'Confusion Matrices', fname))\n",
        "\n",
        "\n",
        "def plot_loss(training_loss, val_loss):\n",
        "    num_epochs = len(training_loss)\n",
        "\n",
        "    plt.clf()\n",
        "    plt.plot(range(1, num_epochs + 1), training_loss, label='Training Loss')\n",
        "    plt.plot(range(1, num_epochs + 1), val_loss, label='Validation Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.xlabel('Num epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    fname = f'loss.png'\n",
        "    plt.savefig(os.path.join('drive', 'MyDrive', 'GenImage', 'results', timestr, fname))\n",
        "\n",
        "\n",
        "def train(train_loss, val_loss, train_acc, val_acc, best_model, epochs, learning_rate, train_accs, val_accs):\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9, last_epoch=-1, verbose=False)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(epochs, config.epochs):\n",
        "        print('-------------------- EPOCH ' + str(epoch) + ' ---------------------')\n",
        "        model.train()\n",
        "        epoch_loss = 0\n",
        "        epoch_accuracy = 0\n",
        "\n",
        "        for step, (inputs, labels) in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):\n",
        "\n",
        "            logits = model(inputs)\n",
        "\n",
        "            if step % 500 == 0:\n",
        "              print(logits)\n",
        "              print(labels)\n",
        "\n",
        "            loss = criterion(logits, labels)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            acc, _ = compute_accuracy(logits, labels)\n",
        "            epoch_accuracy += acc\n",
        "\n",
        "            # Back-propogate\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        # Get train and val loss per batch\n",
        "        epoch_train_loss = epoch_loss / len(train_dataloader)\n",
        "        epoch_train_accuracy = epoch_accuracy / len(train_dataloader)\n",
        "        losses.append(epoch_train_loss)\n",
        "\n",
        "        epoch_val_loss, epoch_val_accuracy = val_model(val_dataloader, model)\n",
        "        val_losses.append(epoch_val_loss)\n",
        "\n",
        "        train_accs.append(epoch_train_accuracy)\n",
        "        val_accs.append(epoch_val_accuracy)\n",
        "\n",
        "        if not val_loss or min(epoch_val_loss, val_loss) == epoch_val_loss:\n",
        "            val_loss = epoch_val_loss\n",
        "            best_model = deepcopy(model.state_dict())\n",
        "        if not train_loss or min(train_loss, epoch_train_loss) == epoch_train_loss:\n",
        "            train_loss = epoch_train_loss\n",
        "        if not train_acc or max(train_acc, epoch_train_accuracy) == epoch_train_accuracy:\n",
        "            train_acc = epoch_train_accuracy\n",
        "        if not val_acc or max(val_acc, epoch_val_accuracy) == epoch_val_accuracy:\n",
        "            val_acc = epoch_val_accuracy\n",
        "\n",
        "        # Adjust learning rate scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        print('Training Loss: ' + str(epoch_train_loss))\n",
        "        print('Validation Loss: ' + str(epoch_val_loss))\n",
        "        print('Training Accuracy: ' + str(epoch_train_accuracy))\n",
        "        print('Validation Accuracy: ' + str(epoch_val_accuracy))\n",
        "        print('---------------------------------------------')\n",
        "\n",
        "        # Save model and stats for checkpoints\n",
        "        save_model(best_model, 'latest_model')\n",
        "        epochs += 1\n",
        "        save_stats(train_loss, val_loss,train_acc, val_acc, epochs,\n",
        "                   scheduler.get_last_lr()[0], train_accs, val_accs)\n",
        "\n",
        "    # Save the model and plot the loss\n",
        "    plot_loss(losses, val_losses)\n",
        "    return train_loss, val_loss, train_acc, val_acc\n",
        "\n",
        "def save_experiment(statistics):\n",
        "    \"\"\"\n",
        "    Saves the experiment results to a csv\n",
        "    :param config: The hyperparameters used\n",
        "    :param statistics: The accuracies for the training, validation, and test sets\n",
        "    \"\"\"\n",
        "    trial_dict = {\n",
        "        'Model name': [timestr],\n",
        "        'Learning rate': [config.learning_rate],\n",
        "        'Weight decay': [config.weight_decay],\n",
        "        'Batch size': [config.batch_size],\n",
        "        'Alpha': [config.alpha],\n",
        "        'Temperature': [config.temperature],\n",
        "        'Epochs': [config.epochs],\n",
        "        'Loss': [config.loss],\n",
        "        'Min Training Loss': [statistics[0]],\n",
        "        'Min Validation Loss': [statistics[1]],\n",
        "        'Maximum Training Accuracy': [statistics[2]],\n",
        "        'Maximum Validation Accuracy': [statistics[3]],\n",
        "    }\n",
        "\n",
        "    trial_dict = pd.DataFrame(trial_dict)\n",
        "    trial_dict.to_csv(os.path.join('drive', 'MyDrive', 'GenImage', 'results',\n",
        "                                   timestr, 'results.csv'), index=False, header=True)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "    checkpoint_path = os.path.join('drive', 'MyDrive', 'GenImage', 'results', timestr)\n",
        "    print(f'All model checkpoints and training stats will be saved in {checkpoint_path}')\n",
        "\n",
        "\n",
        "    losses = []\n",
        "    val_losses = []\n",
        "    train_accs, val_accs = [], []\n",
        "    min_train_loss = None\n",
        "    min_val_loss = None\n",
        "    max_val_acc = None\n",
        "    max_train_acc = None\n",
        "    best_model = None\n",
        "    epochs_ran = 0\n",
        "\n",
        "    model = CLIPLarge(\"laion/CLIP-ViT-H-14-laion2B-s32B-b79K\")\n",
        "    model.to(device)\n",
        "\n",
        "    print_trainable_parameters(model)\n",
        "\n",
        "    # Load datasets\n",
        "    train_dset = ImageDataset(config)\n",
        "    val_dset = ImageDataset(\n",
        "        config,\n",
        "        d_type = 'val'\n",
        "    )\n",
        "\n",
        "    # Create Dataloaders\n",
        "    train_dataloader = DataLoader(train_dset, shuffle=True, batch_size=config.batch_size)\n",
        "    val_dataloader = DataLoader(val_dset, shuffle=True, batch_size=config.batch_size,\n",
        "                                num_workers=config.num_workers)\n",
        "\n",
        "    # Load checkpoint if neccesary:\n",
        "    if config.load_checkpoint:\n",
        "\n",
        "        print('Loading model from ' + config.checkpoint_file)\n",
        "\n",
        "        # Load the model and stats from the checkpoint\n",
        "        model.load_state_dict(torch.load(os.path.join('drive', 'MyDrive', 'GenImage', 'results', config.checkpoint_file,\n",
        "                                                      'latest_model.pth')))\n",
        "        best_model = deepcopy(model)\n",
        "        best_model.load_state_dict(torch.load(os.path.join('drive', 'MyDrive', 'GenImage', 'results', config.checkpoint_file,\n",
        "                                                          'latest_model.pth')))\n",
        "        best_model = best_model.state_dict()\n",
        "\n",
        "        with open(os.path.join('drive', 'MyDrive', 'GenImage', 'results', config.checkpoint_file, 'stats.json'), 'r') as f:\n",
        "            stats = json.load(f)\n",
        "\n",
        "        min_train_loss, min_val_loss, losses, val_losses, epochs_ran = stats['min train loss'], stats[\n",
        "            'min val loss'], stats['losses'], stats['val losses'], stats['epochs']\n",
        "        max_train_acc, max_val_acc = stats['max train acc'], stats['max val acc']\n",
        "        train_accs, val_accs = stats['training accuracies'], stats['val accuracies']\n",
        "\n",
        "        print(f'Minimum Training Loss: {min_train_loss}')\n",
        "        print(f'Training Losses: {losses}')\n",
        "        print(f'Minimum Validation Loss: {min_val_loss}')\n",
        "        print(f'Validation Losses: {val_losses}')\n",
        "        print(f'Training Accuracies: {train_accs}')\n",
        "        print(f'Validation Accuracies: {val_accs}')\n",
        "        print(f'Maximum Training Accuracy: {max_train_acc}')\n",
        "        print(f'Maximum Validation Accuracy: {max_val_acc}')\n",
        "        print(f'Epochs ran: {epochs_ran}')\n",
        "        timestr = config.checkpoint_file\n",
        "    else:\n",
        "        os.mkdir(os.path.join('drive', 'MyDrive', 'GenImage', 'results', timestr))\n",
        "        os.mkdir(os.path.join('drive', 'MyDrive', 'GenImage', 'results', timestr, 'Confusion Matrices'))\n",
        "\n",
        "    # If loading a checkpoint, use the learning rate from the last epoch\n",
        "    if config.load_checkpoint:\n",
        "        lr = stats['learning rate']\n",
        "    else:\n",
        "        lr = config.learning_rate\n",
        "\n",
        "    min_train_loss, min_val_loss, max_train_acc, max_val_acc = (\n",
        "        train(min_train_loss, min_val_loss, max_train_acc,\n",
        "              max_val_acc, best_model, epochs_ran, lr, train_accs, val_accs))\n",
        "    statistics = [min_train_loss, min_val_loss, max_train_acc, max_val_acc]\n",
        "    save_experiment(statistics)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUnwqHn8SRgq"
      },
      "outputs": [],
      "source": [
        "from google.colab import runtime\n",
        "runtime.unassign()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Now9unKVdKwl"
      },
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OhI81p0_6ChV"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5i2J0q88dfR"
      },
      "source": [
        "Make sure to comment out params lines 2318-2323 on /usr/local/lib/python3.10/dist-packages/peft/peft_model.py"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyOECMn1xxmF1RzqCOPt9zlC",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}